{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Admin and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the usual suspects...\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# this is for my bitcoin prices\n",
    "import quandl\n",
    "\n",
    "# visualisation libraries: I'll go ahead and use plotly (offline) for graphing because its d3 \n",
    "# and so I can track prices over time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as sct\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set(style=\"ticks\")\n",
    "sns.set_color_codes(\"muted\")\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "pd.set_option('display.float_format', lambda x:'%.10f' %x)\n",
    "import glob\n",
    "\n",
    "# we'll need this for plotting in nice colours. Trust me on this.\n",
    "COLOR_PALETTE = [    \n",
    "               \"#348ABD\",\n",
    "               \"#A60628\",\n",
    "               \"#7A68A6\",\n",
    "               \"#467821\",\n",
    "               \"#CF4457\",\n",
    "               \"#188487\",\n",
    "               \"#E24A33\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>filecoin</th>\n",
       "      <td>1 Introduction         Filecoin is a protocol ...</td>\n",
       "      <td>The internet is in the middle of a revolution:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gnosis</th>\n",
       "      <td>1.1 Problem Overview           Generally speak...</td>\n",
       "      <td>Prediction markets are poised to become one of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       body  \\\n",
       "filecoin  1 Introduction         Filecoin is a protocol ...   \n",
       "gnosis    1.1 Problem Overview           Generally speak...   \n",
       "\n",
       "                                                    summary  \n",
       "filecoin  The internet is in the middle of a revolution:...  \n",
       "gnosis    Prediction markets are poised to become one of...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white = pd.read_json('tldr_whitepaper_raw.json', orient='index')\n",
    "df_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Split into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use punkt tokeniser as the abstract class for sentence splitting. It is an implmentation of [Unsupervised Multilingual Sentence Boundary Detection (Kiss and Strunk (2005)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf). See [this link](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/init.py#L79) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the punkt tokenizer for sentence splitting\n",
    "# nltk.download('punkt')\n",
    "# print(\"The punkt tokenizer is downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The punkt tokenizer is loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "print(\"The punkt tokenizer is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 543 raw sentences\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = tokenizer.tokenize(df_white.body[0])\n",
    "print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare this to simply splitting by periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reckon we will have numerous spurious sentences formed by phrases such as \"U.S.A.\" ... Lets compare punkt with the naive splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filecoin    898\n",
       "gnosis      867\n",
       "Name: body, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.body.str.split('.').apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice how filecoin has 898 sentence splits identified as opposed to 543 identified by the unsupervised learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use punkt to create a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_white['sentences_body']=df_white.body.apply(lambda x: tokenizer.tokenize(x))\n",
    "df_white['sentences_summary']=df_white.summary.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filecoin    543\n",
       "gnosis      700\n",
       "Name: sentences_body, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.sentences_body.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sentenceBody attributes:  (700, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filecoin_sentenceBody</th>\n",
       "      <th>gnosis_sentenceBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Introduction         Filecoin is a protocol ...</td>\n",
       "      <td>1.1 Problem Overview           Generally speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filecoin protocol provides a data storage and ...</td>\n",
       "      <td>Despite the ease of access we enjoy today, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1 Elementary Components         The Filecoin...</td>\n",
       "      <td>More often than not, the data is severely lack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.</td>\n",
       "      <td>The reason for this is straightforward: writte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decentralized Storage Network (DSN): We provid...</td>\n",
       "      <td>In other words, it’s easy to find what people ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filecoin_sentenceBody  \\\n",
       "0  1 Introduction         Filecoin is a protocol ...   \n",
       "1  Filecoin protocol provides a data storage and ...   \n",
       "2  1.1 Elementary Components         The Filecoin...   \n",
       "3                                                 1.   \n",
       "4  Decentralized Storage Network (DSN): We provid...   \n",
       "\n",
       "                                 gnosis_sentenceBody  \n",
       "0  1.1 Problem Overview           Generally speak...  \n",
       "1  Despite the ease of access we enjoy today, thi...  \n",
       "2  More often than not, the data is severely lack...  \n",
       "3  The reason for this is straightforward: writte...  \n",
       "4  In other words, it’s easy to find what people ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentenceBody = pd.DataFrame(data=[sent for sent in df_white.sentences_body], \n",
    "                               index=df_white.index).T.rename(columns=lambda x: '{:}_{:}'.format(x,'sentenceBody'))\n",
    "print (\"df_sentenceBody attributes: \",df_sentenceBody.shape)\n",
    "df_sentenceBody.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and cleanse sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any character other than A-Za-z0-9 will be culled\n",
    "- Check length of sentence. Cull if too small - could be just headlines or subject identifiers such as \"Introduction\" and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \n",
    "    if type(sentence) is str:\n",
    "        # cleanse all non alpha numeric, non space, non _ or non - \n",
    "        cleaned_sentence = re.sub('[^A-Za-z0-9 _-]', '', sentence)\n",
    "        # convert - or _ into spaces so that you can use n-grams later\n",
    "        cleaned_sentence = re.sub('[^A-Za-z0-9 ]', ' ', cleaned_sentence)\n",
    "        # convert more than one space from all these operations into a single space\n",
    "        cleaned_sentence = re.sub('\\s\\s+', ' ', cleaned_sentence)\n",
    "        # convert to lower case\n",
    "        cleaned_sentence = cleaned_sentence.lower()\n",
    "        # if cleansed sentence contains fewer than 4 words, it probably doesnt contain much info\n",
    "        if len(cleaned_sentence.split(' '))<=3:\n",
    "            cleaned_sentence = np.nan\n",
    "    else:\n",
    "        cleaned_sentence = np.nan\n",
    "            \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data cleansing on every sentence in the body\n",
    "df_sentenceBody_clean = df_sentenceBody.applymap(lambda x: process_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalise the dataset such that you can take idf from every white paper\n",
    "# i achieve this by pivot stacking, then swapping the levels of the multi-index \n",
    "# and finally sorting by multi-index in order\n",
    "df_sentenceBody_clean = df_sentenceBody_clean.stack().swaplevel().sort_index().rename('sentence').reset_index()\n",
    "df_sentenceBody_clean = df_sentenceBody_clean.sort_values(by=['level_0','level_1'],\n",
    "                                    ascending=[True,True]).reindex()\n",
    "df_sentenceBody_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>0</td>\n",
       "      <td>1 introduction filecoin is a protocol token wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>1</td>\n",
       "      <td>filecoin protocol provides a data storage and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>2</td>\n",
       "      <td>11 elementary components the filecoin protocol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>4</td>\n",
       "      <td>decentralized storage network dsn we provide a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>5</td>\n",
       "      <td>later we present the filecoin protocol as an i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0  level_1  \\\n",
       "0  filecoin_sentenceBody        0   \n",
       "1  filecoin_sentenceBody        1   \n",
       "2  filecoin_sentenceBody        2   \n",
       "3  filecoin_sentenceBody        4   \n",
       "4  filecoin_sentenceBody        5   \n",
       "\n",
       "                                            sentence  \n",
       "0  1 introduction filecoin is a protocol token wh...  \n",
       "1  filecoin protocol provides a data storage and ...  \n",
       "2  11 elementary components the filecoin protocol...  \n",
       "3  decentralized storage network dsn we provide a...  \n",
       "4  later we present the filecoin protocol as an i...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentenceBody_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF to manipulate the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161, 6653)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_series_of_sentences = df_sentenceBody_clean.sort_index()['sentence'].as_matrix()\n",
    "count_vect = CountVectorizer(ngram_range=(1, 3),min_df=2)\n",
    "count_vect = count_vect.fit(ordered_series_of_sentences)\n",
    "dtm = count_vect.transform(ordered_series_of_sentences)\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['additional',\n",
       " 'additionally',\n",
       " 'addorders',\n",
       " 'address',\n",
       " 'address derived',\n",
       " 'address derived from',\n",
       " 'adds']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = count_vect.get_feature_names()\n",
    "print (len(features))\n",
    "features[155:162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1161x6653 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "dtm_tfidf = tfidf.fit_transform(dtm)\n",
    "dtm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4282, 3283, 1174, ..., 1594, 5446, 1368])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dtm_tfidf = np.argsort(dtm_tfidf.toarray(), axis=1)[:,::-1]\n",
    "sorted_dtm_tfidf[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise tfidf output rows\n",
    "df_sentenceBody_clean['phrase_with_max_tfidf']=np.nan\n",
    "df_sentenceBody_clean['tfidf_of_phrase_with_max_tfidf']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase_with_max_tfidf</th>\n",
       "      <th>tfidf_of_phrase_with_max_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>0</td>\n",
       "      <td>1 introduction filecoin is a protocol token wh...</td>\n",
       "      <td>proof</td>\n",
       "      <td>0.2285848862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>1</td>\n",
       "      <td>filecoin protocol provides a data storage and ...</td>\n",
       "      <td>miners earn tokens</td>\n",
       "      <td>0.2224005903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>2</td>\n",
       "      <td>11 elementary components the filecoin protocol...</td>\n",
       "      <td>components</td>\n",
       "      <td>0.5762457212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>4</td>\n",
       "      <td>decentralized storage network dsn we provide a...</td>\n",
       "      <td>storage</td>\n",
       "      <td>0.2269995076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>5</td>\n",
       "      <td>later we present the filecoin protocol as an i...</td>\n",
       "      <td>present the filecoin</td>\n",
       "      <td>0.2715922381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0  level_1  \\\n",
       "0  filecoin_sentenceBody        0   \n",
       "1  filecoin_sentenceBody        1   \n",
       "2  filecoin_sentenceBody        2   \n",
       "3  filecoin_sentenceBody        4   \n",
       "4  filecoin_sentenceBody        5   \n",
       "\n",
       "                                            sentence phrase_with_max_tfidf  \\\n",
       "0  1 introduction filecoin is a protocol token wh...                 proof   \n",
       "1  filecoin protocol provides a data storage and ...    miners earn tokens   \n",
       "2  11 elementary components the filecoin protocol...            components   \n",
       "3  decentralized storage network dsn we provide a...               storage   \n",
       "4  later we present the filecoin protocol as an i...  present the filecoin   \n",
       "\n",
       "   tfidf_of_phrase_with_max_tfidf  \n",
       "0                    0.2285848862  \n",
       "1                    0.2224005903  \n",
       "2                    0.5762457212  \n",
       "3                    0.2269995076  \n",
       "4                    0.2715922381  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,d in df_sentenceBody_clean.sort_index().iterrows():\n",
    "    df_sentenceBody_clean.loc[i,'phrase_with_max_tfidf'] = features[sorted_dtm_tfidf[i,0]]\n",
    "    df_sentenceBody_clean.loc[i,'tfidf_of_phrase_with_max_tfidf'] = dtm_tfidf.toarray()[i][sorted_dtm_tfidf[i,0]]\n",
    "df_sentenceBody_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String them into summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase_with_max_tfidf</th>\n",
       "      <th>tfidf_of_phrase_with_max_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>62</td>\n",
       "      <td>is it always f m1</td>\n",
       "      <td>m1</td>\n",
       "      <td>0.6624478622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>135</td>\n",
       "      <td>verifyvk x 01</td>\n",
       "      <td>01</td>\n",
       "      <td>1.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>184</td>\n",
       "      <td>the ledger is append only3</td>\n",
       "      <td>the ledger</td>\n",
       "      <td>0.6999849876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>210</td>\n",
       "      <td>orderbooks are sets of orders</td>\n",
       "      <td>orderbooks</td>\n",
       "      <td>0.6328708081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>filecoin_sentenceBody</td>\n",
       "      <td>521</td>\n",
       "      <td>blockchain archives and inter blockchain stam...</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>0.8821386573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gnosis_sentenceBody</td>\n",
       "      <td>46</td>\n",
       "      <td>3httpwwwnewyorkercomnewsjohn cassidywhat kille...</td>\n",
       "      <td>intrade</td>\n",
       "      <td>0.6546981134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gnosis_sentenceBody</td>\n",
       "      <td>211</td>\n",
       "      <td>massive growth in social media following</td>\n",
       "      <td>growth</td>\n",
       "      <td>0.7779553081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gnosis_sentenceBody</td>\n",
       "      <td>212</td>\n",
       "      <td>surpassed 1000 slack members and 2500 twitter ...</td>\n",
       "      <td>members</td>\n",
       "      <td>0.7078104247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gnosis_sentenceBody</td>\n",
       "      <td>433</td>\n",
       "      <td>used technologies include 1</td>\n",
       "      <td>technologies</td>\n",
       "      <td>0.6588233867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gnosis_sentenceBody</td>\n",
       "      <td>524</td>\n",
       "      <td>a sensor measurement by a specific sensor</td>\n",
       "      <td>sensor</td>\n",
       "      <td>0.8353642481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0  level_1  \\\n",
       "0  filecoin_sentenceBody       62   \n",
       "1  filecoin_sentenceBody      135   \n",
       "2  filecoin_sentenceBody      184   \n",
       "3  filecoin_sentenceBody      210   \n",
       "4  filecoin_sentenceBody      521   \n",
       "5    gnosis_sentenceBody       46   \n",
       "6    gnosis_sentenceBody      211   \n",
       "7    gnosis_sentenceBody      212   \n",
       "8    gnosis_sentenceBody      433   \n",
       "9    gnosis_sentenceBody      524   \n",
       "\n",
       "                                            sentence phrase_with_max_tfidf  \\\n",
       "0                                  is it always f m1                    m1   \n",
       "1                                      verifyvk x 01                    01   \n",
       "2                         the ledger is append only3            the ledger   \n",
       "3                      orderbooks are sets of orders            orderbooks   \n",
       "4   blockchain archives and inter blockchain stam...            blockchain   \n",
       "5  3httpwwwnewyorkercomnewsjohn cassidywhat kille...               intrade   \n",
       "6           massive growth in social media following                growth   \n",
       "7  surpassed 1000 slack members and 2500 twitter ...               members   \n",
       "8                        used technologies include 1          technologies   \n",
       "9          a sensor measurement by a specific sensor                sensor   \n",
       "\n",
       "   tfidf_of_phrase_with_max_tfidf  \n",
       "0                    0.6624478622  \n",
       "1                    1.0000000000  \n",
       "2                    0.6999849876  \n",
       "3                    0.6328708081  \n",
       "4                    0.8821386573  \n",
       "5                    0.6546981134  \n",
       "6                    0.7779553081  \n",
       "7                    0.7078104247  \n",
       "8                    0.6588233867  \n",
       "9                    0.8353642481  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top5 = df_sentenceBody_clean.groupby('level_0')['tfidf_of_phrase_with_max_tfidf'].nlargest(5).reset_index().set_index('level_1')\n",
    "df_top5 = pd.merge(left=df_sentenceBody_clean, right=df_top5)\n",
    "df_top5.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_summary(str_token):\n",
    "    return ' '.join(df_sentenceBody.loc[(df_sentenceBody.index.isin(df_top5[df_top5.level_0==str_token+'_sentenceBody'].level_1))\n",
    "                ,str_token+'_sentenceBody'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it always f = m−1? • Verify(vk, x,π) → 0,1. The ledger is append-only3. Orderbooks are sets of orders. • Blockchain archives and inter-blockchain stamping with Braid.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_summary('filecoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3http://www.newyorker.com/news/john-cassidy/what-killed-intrade 4 https://www.predictit.org/Home/TermsAndConditions                       10 Chapter 1. Massive growth in social media following. Surpassed 1000 slack members and 2500 Twitter followers. Used technologies include:           1. a sensor measurement by a specific sensor.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_summary('gnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haha this is pretty sad. Let's try to make the extractive summaries more legit before jumping into generative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTD:\n",
    "    - REMEMBER TO CHANGE \\N IN THE CLEANSE PROCESS\n",
    "    - INCLUDE MORE PAPERS TO MAKE THE IDF TENABLE\n",
    "    - GET WEIGHTED TF-IDF OF WHOLE SENTENCE AS DESCRIBED IN http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings3/NTCIR3-TSC-SekiY.pdf. USE https://medium.com/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cryptoMe]",
   "language": "python",
   "name": "conda-env-cryptoMe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
